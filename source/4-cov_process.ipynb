{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0274889d-ddef-4d9e-9f4e-3a603d7e8daa",
   "metadata": {},
   "source": [
    "***This notebook calculate patient coverage by 2 steps***\n",
    "1. Get patient 0 position\n",
    "2. Calculate coverage -- output individual coverage file\n",
    "3. Merge coverage files\n",
    "4. Filter the genes in coverage files by gene names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e520c663-0974-4180-808d-a26859aa7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, itertools, time\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bb293c-2bce-49f3-b7bb-c543cdadb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7edb9ee-c473-43b6-af8c-afd7507b94af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define cov_preprocess\n",
    "class cov_process:\n",
    "    def __init__(self, params):\n",
    "        self.dir_wig = params['dir_wig']\n",
    "        self.dir_out = params['dir_out']\n",
    "        self.dir_out_intermediate = params['dir_out_intermediate']\n",
    "        self.dir_out_intermediate_ind = os.path.join(params['dir_out_intermediate'],'individual/')\n",
    "        self.dir_out_intermediate_pos = os.path.join(params['dir_out_intermediate'],'position_dict/')\n",
    "\n",
    "        self.ncore = params['parallelize_core']\n",
    "        ### reference folder and files\n",
    "        self.dir_ref = '../data/proc_refs/'\n",
    "        self.fname_record = 'dict_record_new.pkl'\n",
    "        self.fname_transcript_info = 'dict_transcript_info_062121.pkl'\n",
    "        self.fname_gene_name = 'dict_name_forcov_102121.pkl'  # this is transcript- gene name dict\n",
    "        self.fname_lgene_old = 'gene_name_list_062121.pkl' # this is gene name list only, the old one has 19225 genes \n",
    "        self.fname_lgene_new = 'gene_name_list_102121.pkl' # the new one has 18000ish genes\n",
    "        self.histology_dfname = 'histology.csv'\n",
    "        self.histology_nohype_dfname = 'histology_nohypermutator.csv'\n",
    "        ### output folder and intermediate folder\n",
    "        if not os.path.exists(self.dir_out):\n",
    "            os.makedirs(self.dir_out)\n",
    "        if not os.path.exists(self.dir_out_intermediate):\n",
    "            os.makedirs(self.dir_out_intermediate)\n",
    "        if not os.path.exists(self.dir_out_intermediate_ind):\n",
    "            os.makedirs(self.dir_out_intermediate_ind)\n",
    "        if not os.path.exists(self.dir_out_intermediate_pos):\n",
    "            os.makedirs(self.dir_out_intermediate_pos)\n",
    "        \n",
    "        # get the patient wig file -- as file path-file name list\n",
    "        self.lwig =  os.listdir(self.dir_wig)   #XXX.coverage\n",
    "        self.lwig_fullpath = [os.path.join(self.dir_wig,i) for i in self.lwig] ###../data/cov_preprocess/.../XXX.coverage\n",
    "    ###-------------------------\n",
    "    # 1. First create intermediate file of patient 0 positions\n",
    "    ###-------------------------    \n",
    "    def _get_zero_position(self, coverage_file):\n",
    "        '''\n",
    "       Class method call the below function?\n",
    "        '''\n",
    "        return cov_preprocess.get_zero_position(coverage_file)\n",
    "\n",
    "    @staticmethod\n",
    "    #turn patient wig file into a directory saving 0 positions\n",
    "    def get_zero_position(coverage_file, dir_out):\n",
    "        id_aliquot = coverage_file.split('/')[-1].split('.')[0]\n",
    "        dict_patient = {}\n",
    "        dict_patient[id_aliquot] = {}\n",
    "        with open(coverage_file, 'r+b') as wig_f:\n",
    "            mwig = mmap.mmap(wig_f.fileno(), 0, prot=mmap.PROT_READ)\n",
    "            itmwig = iter(mwig.readline, b\"\")\n",
    "            next(itmwig)\n",
    "            for lines in itmwig:\n",
    "                lines = lines.decode(\"utf-8\") \n",
    "                if lines.startswith('fixed'):\n",
    "                    line_list = re.findall(r'\\d+', lines)\n",
    "                    chr_n = line_list[0]\n",
    "                    start_pos = int(line_list[1])\n",
    "                    position = start_pos - 1\n",
    "                else:\n",
    "                    position += 1\n",
    "                if chr_n not in dict_patient[id_aliquot]:\n",
    "                    dict_patient[id_aliquot][chr_n] = []\n",
    "                if lines == '0\\n':\n",
    "                    dict_patient[id_aliquot][chr_n].append(position)\n",
    "\n",
    "        pickle.dump(dict_patient, open(os.path.join(dir_out,id_aliquot+'.pkl'),'wb'))\n",
    "    \n",
    "    ### This actually call the function to calculate by multiprocessing, output will be saved to intermediate file folder\n",
    "    def run_get_zero_position(self):\n",
    "        print('Start getting patient coverage in genome positions...')\n",
    "        p = Pool(self.ncore)\n",
    "        function1 = partial(cov_preprocess.get_zero_position,dir_out = self.dir_out_intermediate_pos)\n",
    "        tqdm(p.imap(function1, self.lwig_fullpath), total=len(self.lwig_fullpath))\n",
    "        p.close()\n",
    "        p.join()\n",
    "        print('Finish getting patient coverage in genome positions...')\n",
    "\n",
    "    \n",
    "    ###-------------------------\n",
    "    # 2. Now calculate the coverage of patient -- individual coverage file\n",
    "    ###-------------------------\n",
    "    # Load reference data\n",
    "    def load_refs(self):\n",
    "        print('Loading reference data')\n",
    "        self.record = pickle.load(open(os.path.join(self.dir_ref, self.fname_record),'rb'))\n",
    "        self.transcript_info = pickle.load(open(os.path.join(self.dir_ref, self.fname_transcript_info),'rb'))\n",
    "        self.gene_name_dict = pickle.load(open(os.path.join(self.dir_ref, self.fname_gene_name),'rb'))\n",
    "        print('Finish Loading reference data')   \n",
    "        \n",
    "    def _coverage_calculation(self, params):\n",
    "        '''\n",
    "       Class method call the below function?\n",
    "        '''\n",
    "        return cov_preprocess.covarage_calculation(params)\n",
    "\n",
    "        \n",
    "    # Function of calculating coverage\n",
    "    @staticmethod\n",
    "    def coverage_calculation(params, dict_name_forcov,dict_transcript_info, dict_record, pat_id,patient_middlef):  # params:[transcript, patient]        \n",
    "        # get transcript sequence, cdns sequence and positions\n",
    "        strand = dict_transcript_info[params[0]]['strand']\n",
    "        chromosome = dict_transcript_info[params[0]]['chr'].strip('chr')\n",
    "        list_cds = get_mrna_position(params[0], dict_transcript_info, strand)\n",
    "        seq_transcript = get_transcript_sequence(params[0], dict_transcript_info, dict_record, strand)\n",
    "        seq_cds = get_cdna_sequence(list_cds, strand, seq_transcript)\n",
    "        transcript_start_pos = dict_transcript_info[params[0]]['transcript'][0]\n",
    "        dict_position_cov, flag_lp = calculate_coverage(seq_transcript, seq_cds, list_cds, transcript_start_pos)\n",
    "\n",
    "        # Initialize empty dataframe\n",
    "        name_gene = dict_name_forcov[params[0]];zone = ['nonsilent','flank','silent'];categ = [1,2,3,4,5,6,7]\n",
    "        idx = pd.MultiIndex.from_tuples(list(itertools.product([name_gene],zone,categ)))\n",
    "        df_out = pd.DataFrame(index=idx, columns = [pat_id])\n",
    "\n",
    "        for keys in dict_position_cov:\n",
    "            coverage_patient = calculate_patient(dict_position_cov[keys], patient_middlef[params[1]], chromosome)\n",
    "            for i in range(1,8):\n",
    "                df_out.loc[(name_gene,keys,i),:] = coverage_patient[i-1]\n",
    "    \n",
    "        return df_out\n",
    "    \n",
    "    # call function of calculating coverage, it go 1 by 1 patient\n",
    "    def run_coverage_calculation(self, transcript_list):\n",
    "        # Determine the scope of coverage calculation - if all, calculate for all genes\n",
    "        if transcript_list == 'all':\n",
    "            transcript_list = self.gene_name_dict.keys()\n",
    "            \n",
    "        # load patient intermediate file\n",
    "        for p in self.lwig:\n",
    "            pat = p.split('.')[0]\n",
    "            patient_intermediate = pickle.load(open(os.path.join(self.dir_out_intermediate_pos,pat+'.pkl'),'rb')) #intermediate file\n",
    "            list_patient = list(patient_intermediate.keys())\n",
    "            paramlist = list(itertools.product(transcript_list, list_patient)) # parameter list transcipt-patient\n",
    "\n",
    "           # Start processing\n",
    "            start1 = time.time()\n",
    "            # Initialize a list for all dataframes\n",
    "            list_res_cov = []\n",
    "            for param in paramlist[0:3]:\n",
    "                res_cov = cov_preprocess.coverage_calculation(param,self.gene_name_dict,\\\n",
    "                                                              self.transcript_info, self.record,pat,patient_intermediate)\n",
    "                list_res_cov.append(res_cov)\n",
    "            df_res_cov = pd.concat(list_res_cov)\n",
    "            df_res_cov.to_csv(os.path.join(self.dir_out_intermediate_ind,pat+'.csv'), sep = '\\t')\n",
    "            end1 = time.time()\n",
    "            print(f\"finish: {pat}\")\n",
    "            print(f'time used: {end1 - start1}')\n",
    "    \n",
    "    ###-------------------------\n",
    "    # 3. Merge the coverage files by cohort\n",
    "    ###-------------------------    \n",
    "    def load_histology_info(self, gene_name = 'old'):\n",
    "        print('Loading histology cohort & gene reference data')\n",
    "        self.histology_df = pd.read_csv(os.path.join(self.dir_ref,self.histology_dfname))\n",
    "        self.histology_nohype_df = pd.read_csv(os.path.join(self.dir_ref,self.histology_nohype_dfname))\n",
    "        if gene_name == 'old':\n",
    "            self.gene_name_list = pickle.load(open(os.path.join(self.dir_ref, self.fname_lgene_old),'rb'))\n",
    "        elif gene_name == 'new':\n",
    "            self.gene_name_list = pickle.load(open(os.path.join(self.dir_ref, self.fname_lgene_new),'rb'))\n",
    "        print('Finish Loading histology cohort & gene reference data')\n",
    "        \n",
    "    def merge_cov(self, feature, hypermutator = False):\n",
    "        # use different patient list for hypermutator\n",
    "        if hypermutator:\n",
    "            df_sample = self.histology_nohype_df\n",
    "            self.dir_out_merged = self.dir_out+'_nohypermutator'\n",
    "            if not os.path.exists(self.dir_out_merged):\n",
    "                os.makedirs(self.dir_out_merged)\n",
    "        else: \n",
    "            df_sample = self.histology_df\n",
    "            self.dir_out_merged = self.dir_out   \n",
    "\n",
    "        # Initialize list for append\n",
    "        ldf = []\n",
    "\n",
    "#         # Not run if file already exists\n",
    "        # if os.path.exists(os.path.join(self.dir_out_merged,feature+'.csv')):\n",
    "        #     print(f'exists{feature}')\n",
    "        #     return\n",
    "\n",
    "        # Get patient list for histology\n",
    "        df_histology = df_sample[df_sample['histology'] == feature]\n",
    "        lp = df_histology['tumor_aliquot_id'].unique()\n",
    "\n",
    "        #read patient file one by one and append the df in a list\n",
    "        for p in tqdm(lp):\n",
    "            df_cov_ind = pd.read_csv(os.path.join(self.dir_out_intermediate_ind,p+'.csv'),sep = '\\t')\n",
    "            df_cov_ind.columns = ['gene','zone','categ', p]\n",
    "            df_cov_ind = df_cov_ind[df_cov_ind['gene'].isin(self.gene_name_list)] ## Filter genes\n",
    "            df_cov_ind = df_cov_ind.set_index(['gene','zone','categ'])\n",
    "            ldf.append(df_cov_ind)\n",
    "        # pickle.dump(ldf, open(os.path.join(self.dir_out_merged,feature+'.pkl'), 'wb')) #pickle dump the pkl file to prevent dataloss\n",
    "        df_cov_merged = pd.concat(ldf, axis = 1)\n",
    "        df_cov_merged.to_csv(os.path.join(self.dir_out_merged,feature+'.csv.gz'), sep = '\\t'\\\n",
    "         ,chunksize=100000,compression='gzip',encoding='utf-8')\n",
    "        print(f'Finish Merging...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d4d14-61c9-4605-9e5f-40296f30ec78",
   "metadata": {},
   "source": [
    "***example for calculating coverage(individual patient), only for a few genes and 2 patients***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "087d4bbd-331c-4f43-bad0-4a0fcb2086f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### example for calculating coverage, only for a few genes and 2 patients\n",
    "cov_params = {'dir_wig': '../data/cov/example/wig_input',\n",
    "              'dir_out_intermediate': '../data/cov/example/intermediate',\n",
    "              'dir_out': '../data/cov/example/histology',\n",
    "              'parallelize_core':2} \n",
    "\n",
    "## Example run til generating individual cov files\n",
    "## Because you can not really merge individual cov files if you don't have a cohort\n",
    "res = cov_process(cov_params)\n",
    "res.run_get_zero_position()\n",
    "res.load_refs()\n",
    "res.run_coverage_calculation('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8156b-2f9b-43dc-9584-acaa34decbe4",
   "metadata": {},
   "source": [
    "***Merge coverage into histology cohort***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099edf3d-5a6d-41f0-9e47-5029ee9cba29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/109 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ovary-AdenoCA' 'CNS-PiloAstro' 'Liver-HCC' 'CNS-Oligo' 'Panc-Endocrine'\n",
      " 'Kidney-RCC' 'Prost-AdenoCA' 'Thy-AdenoCA' 'ColoRect-AdenoCA'\n",
      " 'Lymph-BNHL' 'Uterus-AdenoCA' 'Breast-AdenoCA' 'Lung-AdenoCA'\n",
      " 'Panc-AdenoCA' 'Eso-AdenoCA' 'Head-SCC' 'CNS-Medullo' 'CNS-GBM'\n",
      " 'SoftTissue-Leiomyo' 'Cervix-SCC' 'Skin-Melanoma' 'Lymph-CLL'\n",
      " 'SoftTissue-Liposarc' 'Kidney-ChRCC' 'Stomach-AdenoCA' 'Lung-SCC'\n",
      " 'Bladder-TCC' 'Myeloid-AML' 'Biliary-AdenoCA' 'Breast-LobularCA'\n",
      " 'Cervix-AdenoCA' 'Bone-Osteosarc' 'Breast-DCIS' 'Myeloid-MPN'\n",
      " 'Myeloid-MDS' 'Bone-Cart' 'Bone-Osteoblast' 'Bone-Epith' 'Bone-Benign']\n",
      "Loading histology cohort & gene reference data\n",
      "Finish Loading histology cohort & gene reference data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:32<00:00,  3.38it/s]\n",
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:27<00:00,  3.26it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [01:29<00:00,  3.49it/s]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.60it/s]\n",
      "  0%|          | 0/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:23<00:00,  3.46it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:41<00:00,  3.47it/s]\n",
      "  0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:57<00:00,  3.48it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Merging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 23/48 [00:06<00:07,  3.48it/s]"
     ]
    }
   ],
   "source": [
    "#### If you already have individual patient file, continue to merge them according to histology types\n",
    "### merging real coverage data\n",
    "# read feature list - e.g. All the histology tyeps\n",
    "df_cohort = pd.read_csv(os.path.join('../data/proc_refs/histology.csv'))\n",
    "lfeat = df_cohort['histology'].unique()\n",
    "print(lfeat)\n",
    "\n",
    "# running class and load information\n",
    "cov_params = {'dir_wig': '../data/cov/example/wig_input',\n",
    "              'dir_out_intermediate': '../data/cov/intermediate',\n",
    "              'dir_out': '../data/cov/histology_new',\n",
    "              'parallelize_core':2} \n",
    "\n",
    "res = cov_process(cov_params)\n",
    "res.load_histology_info(gene_name = 'new')\n",
    "\n",
    "for histologies in lfeat:\n",
    "    res.merge_cov(histologies, hypermutator = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
